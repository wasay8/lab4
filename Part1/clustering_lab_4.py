# -*- coding: utf-8 -*-
"""Clustering_lab_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zTo5VUwnVvxiZuhxyijeoFntNeldzpHu

# Libraries
"""

import re
from urllib.parse import urlparse

import matplotlib.pyplot as plt
import nltk
import pandas as pd
import seaborn as sns
import spacy
from gensim.models import Word2Vec
from sklearn.cluster import KMeans

"""# Lab 4 Part 1"""
def Preprocessing(data):
	data.drop_duplicates(inplace = True)

	data.drop_duplicates(subset=['Title', 'Post URL', 'Total Comments', 'ID'], inplace = True)

	try:
		data.drop("Unnamed: 0", axis = 1, inplace = True)
	except:
		pass

	# Function to extract the domain from the URL
	def extract_domain(url):
		try:
			parsed_url = urlparse(url)
			# print(parsed_url)
			domain = parsed_url.netloc
			# print(domain)
			if domain.startswith('www.'):
			    domain = domain[4:]  # Remove 'www.' if present
			if domain.endswith(".com"):
			    domain = domain[:-4]
			# print(domain)
			return domain
		except Exception as e:
			print(f"Error extracting domain: {e}")
			return None

	# Create a new column 'domain_url' with the extracted domains
	data['Domain URL'] = data['Post URL'].apply(extract_domain) # Now, you have a DataFrame with a new 'domain_url' column containing "theverge.com"

	"""## Title: Keywords and Topics"""

	# Initialize NLTK stopwords and spaCy NLP model
	nltk.download('stopwords')
	stop_words = set(nltk.corpus.stopwords.words('english'))

	if not spacy.util.is_package("en_core_web_sm"):
		spacy.cli.download("en_core_web_sm")

	nlp = spacy.load('en_core_web_sm')

	def extract_keywords_and_topics(title):

	    # Tokenize and process the title with spaCy
	    doc = nlp(title)

	    # Extract keywords (non-stop words and non-punctuation)
	    keywords = [token.text.lower() for token in doc if token.text.lower() not in stop_words and not token.is_punct]

	    # Extract topics (noun phrases)
	    topics = [chunk.text for chunk in doc.noun_chunks]

	    return {
		'Keywords': ', '.join(keywords),
		'Topics': ', '.join(topics)
	    }

	# Apply the function to each title in the DataFrame
	data[['Keywords', 'Topics']] = data['Title'].apply(extract_keywords_and_topics).apply(pd.Series)

	return data

"""# Lab 4 pat-2

## Clustering Algo
"""
# Modify the document_vector function to pad vectors with zeros
def document_vector(model, keywords, vec_dim=100):
	doc_vector = [model.wv[word] for word in keywords if word in model.wv]
	if doc_vector:
		padded_vector = sum(doc_vector) / len(doc_vector)
		return padded_vector
	else:
		# If no valid word vectors found, return a vector of zeros
		return [0.0] * vec_dim
def clustering(data,clusterd_method,num_clusters):
	# Step 1: Tokenize and preprocess the text
	sentences = [keywords.split() for keywords in data['Keywords']]

	# Step 2: Train a Word2Vec model
	model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)  # Adjust parameters as needed


	# Apply the modified document_vector function
	data['vector'] = data['Keywords'].apply(lambda x: document_vector(model, x))

	# Continue with the clustering step
	kmeans = clusterd_method
	data['cluster'] = kmeans.fit_predict(list(data['vector']))

	# Step 5: Display clusters using a visualization tool
	plt.ion()
	plt.figure(figsize=(10, 6))

	plt.title('Cluster Distribution')
	plt.xlabel('Cluster')
	plt.ylabel('Count')
	sns.countplot(data=data, x='cluster')
	plt.pause(2)



	# Storing Clusters and keywords
	cluster_keywords = {}

	for cluster_id in range(num_clusters):
	  cluster_keywords[cluster_id] = data[data["cluster"]==cluster_id]["Keywords"].to_list()

	for cluster_id, keywords in cluster_keywords.items():
	    print(f"Cluster: {cluster_id} Keywords: {keywords}")

	"""### Elbow method : for Numbers of Cluster"""

	# Continue with the clustering step
	# Try different values of 'k' and calculate the WCSS for each value
	wcss = []

	# Define a range of values for 'k' to test
	k_values = range(1, 11)  # You can adjust this range as needed

	for k in k_values:
	    kmeans = KMeans(n_clusters=k)
	    kmeans.fit(list(data['vector']))
	    wcss.append(kmeans.inertia_)

	# Plot the WCSS values
	plt.figure(figsize=(10, 6))

	plt.title('Elbow Method')
	plt.xlabel('Number of Clusters (k)')
	plt.ylabel('Within-Cluster Sum of Squares (WCSS)')
	plt.grid(True)
	plt.plot(k_values, wcss, marker='o', linestyle='-')
	plt.pause(2)
	plt.ioff()
	# You can visually inspect the plot to determine the optimal number of clusters (k)
	return model,data

"""## Predicting Clusters"""

# Function to find the closest cluster to input keywords or message
def find_closest_cluster(input_text,clusterd_model,vec_model):
    input_vector = document_vector(vec_model, input_text.split())
    if len(input_vector) > 0:
        closest_cluster = clusterd_model.predict([input_vector])[0]
        return closest_cluster
    else:
        return None

# Example usage:
if __name__ =="__main__":
	data = pd.read_csv("Posts_v2.csv")
	data = Preprocessing(data)
	kmeans = KMeans(n_clusters=10)
	model,data = clustering(data,kmeans,10)
	input_text = input("Please enter a message or keywords to Cluster:")
	closest_cluster = find_closest_cluster(input_text,kmeans,model)
	print(closest_cluster)
	# Display messages from the selected cluster
	if closest_cluster is not None:
		cluster_messages = data[data['cluster'] == closest_cluster]['Keywords']
		print(data.head(5))
		print(f"Messages in Cluster {closest_cluster}:")
		for message in cluster_messages:
			print(f"- {message}")

		# Visualize the cluster
		cluster_df = data[data['cluster'] == closest_cluster]
		plt.figure(figsize=(10, 6))
		sns.countplot(data=cluster_df, x='cluster')
		plt.title(f'Cluster {closest_cluster} Distribution')
		plt.xlabel('Cluster')
		plt.ylabel('Count')
		print(plt.show())
	else:
	    	print("No cluster found for the input text.")

"""import pandas as pd
import re
import urllib
import spacy
import nltk
import gensim
import sklearn
import matplotlib
import seaborn

# Check library versions
print("Pandas version:", pd.__version__)
print("Regex (re) version:", re.__version__)
# print("urllib.parse version:", urllib.__version__)
print("spaCy version:", spacy.__version__)
print("NLTK version:", nltk.__version__)
print("Gensim version:", gensim.__version__)
print("scikit-learn (sklearn) version:", sklearn.__version__)
print("Matplotlib version:", matplotlib.__version__)
print("Seaborn version:", seaborn.__version__)"""
